---
title: 基于强化学习的多无人机协同围捕算法——MADDPG Multi-UAV Roundup
date: 2025-02-04 14:18:08
tags:
    - 强化学习
categories: 
    - 强化学习
description: |
    基于强化学习的多无人及协同围捕算法——MADDPG 的理论与仿真代码解读。
---

MADDPG（Multi-Agent Deep Deterministic Policy Gradient）是一种针对多智能体强化学习环境的算法，扩展自DDPG（Deep Deterministic Policy Gradient）。其核心思想是**集中式训练、分散式执行**，通过利用全局信息优化策略，同时保持智能体在执行时的独立性。

---

### **核心思想**
1. **非平稳环境问题**：多智能体环境中，每个智能体的策略变化会导致其他智能体的环境动态不稳定，传统单智能体算法难以适应。
2. **集中训练，分散执行**：
   - **训练阶段**：每个智能体的Critic网络能访问所有智能体的状态和动作信息，从而更准确估计Q值。
   - **执行阶段**：每个智能体的Actor仅依赖局部观测生成动作，无需全局信息。

---

### **算法结构**
1. **网络组成**：
   - **每个智能体包含四个网络**：
     - **Actor网络（μ）**：基于局部观测生成动作。
     - **Critic网络（Q）**：输入所有智能体的状态和动作，输出Q值。
     - **目标Actor网络（μ'）和Critic网络（Q'）**：用于稳定训练，参数通过软更新同步。

2. **输入输出**：
   - **Actor**：输入局部观测（s_i），输出动作（a_i）。
   - **Critic**：输入全局状态（s_1, s_2, ..., s_N）和所有动作（a_1, a_2, ..., a_N），输出Q值。

---

### **训练流程**
1. **经验回放池**：
   - 存储所有智能体的经验元组（s, a, r, s'），其中s和a为全局信息。
   
2. **采样与更新**：
   - 从回放池中采样一批经验。
   - 对每个智能体i：
     - **计算目标Q值**

     - **更新Critic**：最小化TD误差的均方损失

     - **更新Actor**：通过梯度上升最大化Q值

     - **软更新目标网络**


---

### **关键优势**
1. **解决非平稳性**：Critic使用全局信息，使Q值估计更稳定。
2. **适用性广泛**：支持合作、竞争或混合任务，智能体可独立优化自身目标。
3. **扩展性**：适用于连续动作空间，智能体数量可变（需适当调整Critic输入维度）。

---

### **挑战与改进**
1. **输入维度爆炸**：智能体数量增加时，Critic输入维度可能过高。解决方案包括参数共享或注意力机制。
2. **异构智能体**：若智能体动作/状态空间不同，需设计统一输入表示（如拼接或编码）。
3. **策略过拟合**：引入策略集成（Policy Ensembles）或对手建模，提升对其他智能体策略变化的鲁棒性。

---

### **应用场景**
- **合作任务**：如多机器人协作搬运、群体围捕。
- **竞争任务**：如博弈对抗（足球游戏、格斗游戏）。
- **混合任务**：部分合作、部分竞争的环境（如市场竞争与联盟形成）。

---

### **总结**
MADDPG通过集中式Critic网络解决了多智能体环境中的非平稳性问题，同时保持执行阶段的分布式特性。其核心在于利用全局信息优化局部策略，适用于复杂多智能体场景，是MARL（Multi-Agent Reinforcement Learning）领域的里程碑算法。

## 奖励函数
### 1. 撞墙惩罚：
1. 靠近墙体： **-1 * 0.3**
2. 撞到墙体： **-500 * 0.3**

### 2. 避障惩罚：
1. 靠近障碍物： **-1 * 0.3**
2. 撞到障碍物： **-500 * 0.3**

### 3. 捕捉奖励：
1. 捉到 target： **1000 * 5**
2. 否则： **-2 * d * 5** (0<d<2)

### 4. 编队奖励：
1. follower 超出范围： follower **- 10 * d * 0.8**

### 5. 速度协同奖励：
速度协同：
1. leader + **1*0.1**
2. follower + **1*0.8**