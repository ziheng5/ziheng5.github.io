---
title: LSTM åŸç†åŠå…¶ PyTorch å®ç°
date: 2024-12-13 13:00:36
tags: 
    - PyTorch
    - æ·±åº¦å­¦ä¹ 
    - RNN
categories:
    - æ·±åº¦å­¦ä¹ 
description: |
    ğŸ“– ä½¿ç”¨ PyTorch å®ç° LSTM
---
> PyTorch å…¶å®å†…ç½®äº† LSTM æ¨¡å‹ï¼Œç›´æ¥è°ƒç”¨å³å¯ï¼Œä¸éœ€è¦è´¹åŠ²å»æ‰‹æ“äº†
>
> ï¼ˆæŸä¸ªäººå¤ç°åˆ°ä¸€åŠæ‰ååº”è¿‡æ¥ ğŸ˜­ï¼‰
>
> âœ¨é€šä¿—æ˜“æ‡‚çš„ LSTM åŸç†è®²è§£ï¼ˆåŠ›æ¨ï¼‰ï¼š
> https://www.youtube.com/watch?v=YCzL96nL7j0&t=1s
>
> ï¼ˆ å‘æ˜ LSTM çš„äººçœŸ ** æ˜¯ä¸ªå¤©æ‰ï¼ï¼‰

## 1. â“ ä»€ä¹ˆæ˜¯ LSTM
é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼ŒLong-Short-Term Memoryï¼‰æ˜¯ä¼ ç»Ÿ RNN ç½‘ç»œçš„ Plus ç‰ˆæœ¬ã€‚
### 1.1 å‘æ˜èƒŒæ™¯
ä¼ ç»Ÿçš„ RNN ç½‘ç»œåœ¨è®­ç»ƒçš„æ—¶å€™ï¼Œå½“é‡åˆ°é•¿åºåˆ—æ•°æ®æ—¶ï¼Œå¾ˆå®¹æ˜“å‡ºç° **æ¢¯åº¦çˆ†ç‚¸** ä¸ **æ¢¯åº¦æ¶ˆå¤±** çš„æƒ…å†µï¼Œå¯¼è‡´è®­ç»ƒæ•ˆæœä¸å¤ªå¥½ã€‚

> ğŸ‘€ ä»€ä¹ˆï¼Ÿä½ ä¸çŸ¥é“ä»€ä¹ˆæ˜¯ **æ¢¯åº¦çˆ†ç‚¸** å’Œ **æ¢¯åº¦æ¶ˆå¤±**ï¼Ÿï¼å¿«æ¥çœ‹çœ‹è¿™ä¸ªè§†é¢‘ï¼š
> 
> https://www.youtube.com/watch?v=AsNTP8Kwu80

ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼ŒLSTM åœ¨ä¼ ç»Ÿ RNN çš„åŸºç¡€ä¸Šï¼ŒåŠ å…¥äº† **é—¨æ§æœºåˆ¶ï¼ˆGateï¼‰** æ¥æ§åˆ¶ä¿¡æ¯æµåŠ¨ï¼Œä»è€Œè®°ä½é•¿æœŸä¾èµ–ä¿¡æ¯ã€‚

### 1.2 åŸç†è¯¦è§£
> å…ˆçœ‹è§†é¢‘ï¼šhttps://www.youtube.com/watch?v=YCzL96nL7j0&t=1s

LSTM ç”±å¤šä¸ª **LSTM å•å…ƒï¼ˆCellï¼‰** ç»„æˆï¼Œæ¯ä¸ªå•å…ƒåŒ…å«ä»¥ä¸‹ä¸‰ä¸ªé—¨å’Œä¸€ä¸ªå•å…ƒçŠ¶æ€ï¼š  

#### **é—å¿˜é—¨ï¼ˆForget Gateï¼‰**
- **åŠŸèƒ½ï¼š** å†³å®šå“ªäº›ä¿¡æ¯éœ€è¦ **é—å¿˜**ã€‚
- **å…¬å¼ï¼š**  
$$ f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) $$
- **è§£é‡Šï¼š**
  - è¾“å…¥ï¼šå‰ä¸€ä¸ªéšè—çŠ¶æ€ $h_{t-1}$ å’Œå½“å‰è¾“å…¥ $x_t$ã€‚
  - è¾“å‡ºï¼šèŒƒå›´åœ¨ $[0, 1]$ï¼Œå…¶ä¸­ 0 è¡¨ç¤ºå®Œå…¨é—å¿˜ï¼Œ1 è¡¨ç¤ºå®Œå…¨ä¿ç•™ã€‚  

#### **è¾“å…¥é—¨ï¼ˆInput Gateï¼‰**
- **åŠŸèƒ½ï¼š** å†³å®šå“ªäº›æ–°ä¿¡æ¯éœ€è¦ **å­˜å‚¨**ã€‚
- **å…¬å¼ï¼š**
  - å€™é€‰ä¿¡æ¯ç”Ÿæˆï¼š  
    $$ \tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$$
  - è¾“å…¥é—¨æ¿€æ´»ï¼š  
    $$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$$
  - æ›´æ–°å•å…ƒçŠ¶æ€ï¼š  
    $$C_t = f_t \cdot C_{t-1} + i_t \cdot \tilde{C}_t$$
- **è§£é‡Šï¼š**
  - å€™é€‰ä¿¡æ¯ $\tilde{C}_t$ï¼šå½“å‰æ—¶é—´æ­¥çš„æ–°ä¿¡æ¯ã€‚
  - è¾“å…¥é—¨ $i_t$ï¼šæ§åˆ¶å€™é€‰ä¿¡æ¯çš„å­˜å‚¨ç¨‹åº¦ã€‚  

#### **è¾“å‡ºé—¨ï¼ˆOutput Gateï¼‰**
- **åŠŸèƒ½ï¼š** å†³å®šå•å…ƒçŠ¶æ€ä¸­çš„ä¿¡æ¯ **å…¬å¼€è¾“å‡º**ã€‚
- **å…¬å¼ï¼š**
  - è¾“å‡ºé—¨æ¿€æ´»ï¼š  
    $$o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$$
  - æœ€ç»ˆéšè—çŠ¶æ€ï¼š  
    $$h_t = o_t \cdot \tanh(C_t)$$
- **è§£é‡Šï¼š**
  - è¾“å‡ºé—¨å†³å®šå½“å‰æ—¶é—´æ­¥çš„éšè—çŠ¶æ€ $h_t$ï¼Œè¯¥çŠ¶æ€å°†ä½œä¸ºä¸‹ä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å…¥ã€‚

#### **å•å…ƒçŠ¶æ€ï¼ˆCell Stateï¼‰**
- **åŠŸèƒ½ï¼š**  
  - ä½œä¸ºä¿¡æ¯çš„â€œé•¿æœŸè®°å¿†â€è·¯å¾„ï¼Œåœ¨æ•´ä¸ªæ—¶é—´åºåˆ—ä¸­æµåŠ¨ã€‚
  - çº¿æ€§ä¼ é€’ï¼Œå‡ ä¹ä¸å—æ¿€æ´»å‡½æ•°çš„å½±å“ï¼Œç¡®ä¿é•¿æ—¶é—´çš„ä¿¡æ¯ä¿ç•™ã€‚

### 1.3 LSTM æ•°æ®æµæ€»ç»“ï¼š
1. æ¥æ”¶è¾“å…¥ $x_t$ å’Œä¸Šä¸€ä¸ªéšè—çŠ¶æ€ $h_{t-1}$ã€‚  
2. **é—å¿˜é—¨** ç¡®å®šéœ€è¦é—å¿˜çš„ä¿¡æ¯ã€‚  
3. **è¾“å…¥é—¨** ç¡®å®šå­˜å‚¨çš„æ–°ä¿¡æ¯ã€‚  
4. æ›´æ–°å•å…ƒçŠ¶æ€ $C_t$ã€‚  
5. **è¾“å‡ºé—¨** ç¡®å®šå½“å‰æ—¶é—´æ­¥çš„è¾“å‡ºéšè—çŠ¶æ€ $h_t$ã€‚  

### 1.4 LSTM çš„ä¼˜åŠ¿ï¼š
- **é•¿æœŸä¾èµ–è®°å¿†ï¼š** èƒ½å¤Ÿæœ‰æ•ˆè®°ä½é•¿æœŸä¿¡æ¯ï¼Œè§£å†³äº†ä¼ ç»Ÿ RNN çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚
- **é€‚ç”¨åœºæ™¯ï¼š** å¹¿æ³›ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ã€æ—¶é—´åºåˆ—é¢„æµ‹ã€è¯­éŸ³è¯†åˆ«ç­‰é¢†åŸŸã€‚  
- **çµæ´»æ€§é«˜ï¼š** æ”¯æŒå¤šå±‚å †å ï¼Œèƒ½å¤Ÿå­¦ä¹ é«˜åº¦å¤æ‚çš„æ•°æ®æ¨¡å¼ã€‚  

---

## 2. PyTorch æ‰‹åŠ¨å¤ç° LSTMï¼ˆçµæ´»åº¦é«˜ï¼‰
> å‚è€ƒæ•™ç¨‹ï¼šhttps://www.youtube.com/watch?v=RHGiXPuo_pI
### 2.1 ğŸ“¦å¯¼å…¥åŒ…
```Python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Adam

import lightning as L
from torch.utils.data import TensorDataset, DataLoader
```

âš ï¸æ³¨æ„ï¼šè¿™é‡Œæœ‰ä¸ªå« `lightning` çš„åŒ…ã€‚

> ä»€ä¹ˆï¼Ÿä½ ä¸çŸ¥é“è¿™ä¸ªåŒ…æ˜¯ç”¨æ¥å¹²ä»€ä¹ˆçš„ï¼Ÿï¼PyTorch Lightning æ˜¯ä¸€ä¸ªåŸºäº PyTorch çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå…¶åŠŸèƒ½ç›¸å½“å¼ºå¤§ï¼Œå¯ä»¥ä¸€é”®å®ç°å¾ˆå¤šåŠŸèƒ½ï¼
>
> å®˜æ–¹æ–‡æ¡£ï¼šhttps://lightning.ai/docs/pytorch/stable/
>
> ä¸‹è½½æ–¹å¼:
>
> 1. pip ç”¨æˆ·ï¼š
> ```Terminal
> pip install lightning
> ```
> 2. conda ç”¨æˆ·ï¼š
> ```Terminal
> conda install lightning
> ```

### 2.2 âœ‹æ‰‹æ“ LSTM ç½‘ç»œ
```Python
class LSTMbyHand(L.LightningModule):
    def __init__(self):
        # create and initialize weight and bias tensors
        super().__init__()
        mean = torch.tensor(0.0)
        std = torch.tensor(1.0)

        ## 1. é—å¿˜é—¨ 
        self.wlr1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)
        self.wlr2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)
        self.blr1 = nn.Parameter(torch.tensor(0.), requires_grad=True)

        ## 2. è¾“å…¥é—¨
        self.wpr1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)
        self.wpr2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)
        self.bpr1 = nn.Parameter(torch.tensor(0.), requires_grad=True)

        self.wp1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)
        self.wp2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)
        self.bp1 = nn.Parameter(torch.tensor(0.), requires_grad=True)

        ## 3. è¾“å‡ºé—¨
        self.wo1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)
        self.wo2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)
        self.bo1 = nn.Parameter(torch.tensor(0.), requires_grad=True)


    def lstm_unit(self, input_value, long_memory, short_memory):
        # do the lstm math
        ## 1. é—å¿˜é—¨
        long_remember_percent = torch.sigmoid((short_memory * self.wlr1) +
                                              (input_value * self.wlr2) +
                                              self.blr1)
        
        ## 2. è¾“å…¥é—¨
        potential_remember_percent = torch.sigmoid((short_memory * self.wpr1) +
                                                   (input_value * self.wpr2) +
                                                   self.bpr1)
        
        potential_memory = torch.tanh((short_memory * self.wp1) +
                                      (input_value * self.wp2) +
                                      self.bp1)
        
        updated_long_memory = ((long_memory * long_remember_percent) +
                              (potential_memory * potential_remember_percent))
        
        ## 3. è¾“å‡ºé—¨
        output_percent = torch.sigmoid((short_memory * self.wo1) +
                                       (input_value * self.wo2) +
                                       self.bo1)
        
        updated_short_memory = torch.tanh(updated_long_memory) * output_percent

        ## 4. è¾“å‡º
        return ([updated_long_memory, updated_short_memory])


    def forward(self, input):
        # make a forward pass through unrolled lstm
        long_memory = 0
        short_memory = 0
        day1 = input[0]
        day2 = input[1]
        day3 = input[2]
        day4 = input[3]

        long_memory, short_memory = self.lstm_unit(day1, long_memory, short_memory)
        long_memory, short_memory = self.lstm_unit(day2, long_memory, short_memory)
        long_memory, short_memory = self.lstm_unit(day3, long_memory, short_memory)
        long_memory, short_memory = self.lstm_unit(day4, long_memory, short_memory)

        return short_memory


    def configure_optimizers(self):
        # configure adam optimizer
        return Adam(self.parameters())
    

    def training_step(self, batch, batch_idx):
        # calculate loss and log training progress
        input_i, label_i = batch
        output_i = self.forward(input_i[0])
        loss = (output_i - label_i) ** 2

        self.log("train_loss", loss)
        
        if (label_i == 0):
            self.log("out_0", output_i)
        else:
            self.log("out_1", output_i)

        return loss
```

### 2.3 ğŸ”æ£€æŸ¥ç½‘ç»œæ˜¯å¦æ­£ç¡®æ­å»º
```Python
model = LSTMbyHand()

print("\nNow let's compare the observed and predicted values...")
print("Company A: Observed = 0, Predicted = ", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())

print("Company B: Observed = 1, Predicted = ", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())
```

### 2.4 ğŸ’ªå¼€å§‹è®­ç»ƒ
```Python
inputs = torch.tensor([[0., 0.5, 0.25, 1.], [1., 0.5, 0.25, 1.]])
labels = torch.tensor([0., 1.])

dataset = TensorDataset(inputs, labels)
dataloader = DataLoader(dataset)

trainer = L.Trainer(max_epochs=2000)
trainer.fit(model, train_dataloaders=dataloader)
```

### 2.5 ğŸ”æ£€æŸ¥è®­ç»ƒæ•ˆæœ
```Terminal
tensorboard --logdir=lightning_logs/
```
![result](./images/lstm/tensorboard1.png)

å‘ç°æ•ˆæœä¸€èˆ¬ğŸ˜¢

### 2.6 ğŸ’ªè¿ç§»å­¦ä¹ 
```Python
path_to_best_checkpoint = trainer.checkpoint_callback.best_model_path

trainer = L.Trainer(max_epochs=5000)
trainer.fit(model, train_dataloaders=dataloader, ckpt_path=path_to_best_checkpoint)
```
å†æ¬¡æŸ¥çœ‹æ•ˆæœï¼š

![result2](./images/lstm/tensorboard3.png)

æ•ˆæœå·¨å¥½ğŸ‘Œ

## 3. PyTorch å†…ç½® LSTM çš„ä½¿ç”¨
### 3.1 ğŸ“¦å¯¼å…¥åŒ…
```Python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Adam

import lightning as L
from torch.utils.data import TensorDataset, DataLoader
```

### 3.2 ğŸ§±æ­å»ºç½‘ç»œ
```Python
class LightningLSTM(L.LightningModule):
    def __init__(self):
        super().__init__()
        self.lstm = nn.LSTM(input_size=1, hidden_size=1)

    
    def forward(self, input):
        input_trans = input.view(len(input), 1)

        lstm_out, temp = self.lstm(input_trans)

        prediction = lstm_out[-1]
        return prediction
    

    def configure_optimizers(self):
        return Adam(self.parameters(), lr=0.1)
    

    def training_step(self, batch, batch_idx):
        input_i, label_i = batch
        output_i = self.forward(input_i[0])
        loss = (output_i - label_i) ** 2

        self.log("train_loss", loss)

        if (label_i==0):
            self.log("out_0", output_i)
        else:
            self.log("out_1", output_i)

        return loss
```
è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨ PyTorch ä¸­ï¼ŒLSTM çš„è¾“å…¥æ ¼å¼ä¸º `(batch_size, sequence_length, input_size)`ï¼Œå³**æ ·æœ¬æ•°é‡ã€æ—¶é—´æ­¥é•¿ã€ç‰¹å¾æ•°é‡**ã€‚

å¦‚æœå®é™…ä¸­çš„æ•°æ®æ ¼å¼ä¸æ­¤ä¸åŒï¼Œéœ€è¦æ‰‹åŠ¨å¤„ç†æˆå¯¹åº”çš„æ ¼å¼ã€‚


### 3.3 ğŸ’ªå¼€å§‹è®­ç»ƒ
```Python
model = LightningLSTM()

trainer = L.Trainer(max_epochs=300, log_every_n_steps=2)
trainer.fit(model, train_dataloaders=dataloader)
```