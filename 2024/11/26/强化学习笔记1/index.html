
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8" />
    <title>强化学习理论笔记1——基本概念 | Cold Rain&#39;s Blog</title>
    <meta name="author" content="ColdRain" />
    <meta name="description" content="銀の龍の背に乗って..." />
    <meta name="keywords" content="" />
    <meta
        name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0"
    />
    <link rel="icon" href="/images/head.jpg" />
    <link rel="preconnect" href="https://s4.zstatic.net" />
<script src="https://s4.zstatic.net/ajax/libs/vue/3.3.7/vue.global.prod.min.js"></script>
<link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/font-awesome/6.4.2/css/all.min.css" />
<link rel="preconnect" href="https://fonts.googleapis.cn" />
<link rel="preconnect" href="https://fonts.gstatic.cn" crossorigin />
<link
    rel="stylesheet"
    href="https://fonts.googleapis.cn/css2?family=Fira+Code:wght@400;500;600;700&family=Lexend:wght@400;500;600;700;800;900&family=Noto+Sans+SC:wght@400;500;600;700;800;900&display=swap"
/>
<script> const mixins = {}; </script>

<script src="https://polyfill.alicdn.com/v3/polyfill.min.js?features=default"></script>


<script src="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script src="https://s4.zstatic.net/ajax/libs/highlightjs-line-numbers.js/2.8.0/highlightjs-line-numbers.min.js"></script>
<link
    rel="stylesheet"
    href="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/styles/vs.min.css"
/>
<script src="/js/lib/highlight.js"></script>


<script src="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/katex.min.js"></script>
<script src="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/contrib/auto-render.min.js"></script>
<link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/katex.min.css" />
<script src="/js/lib/math.js"></script>


<script src="/js/lib/preview.js"></script>









<link rel="stylesheet" href="/css/main.css" />

<meta name="generator" content="Hexo 7.3.0"></head>
<body>
    <div id="layout">
        <transition name="fade">
            <div id="loading" v-show="loading">
                <div id="loading-circle">
                    <h2>LOADING</h2>
                    <p>加载过慢请开启缓存 浏览器默认开启</p>
                    <img src="/images/loading.gif" />
                </div>
            </div>
        </transition>
        <div id="menu" :class="{ hidden: hiddenMenu, 'menu-color': menuColor}">
    <nav id="desktop-menu">
        <a class="title" href="/">
            <span>COLD RAIN&#39;S BLOG</span>
        </a>
        
        <a href="/">
            <i class="fa-solid fa-house fa-fw"></i>
            <span>&ensp;Homepage</span>
        </a>
        
        <a href="/2024/11/26/test">
            <i class="fa-solid fa-id-card fa-fw"></i>
            <span>&ensp;About</span>
        </a>
        
        <a href="/archives">
            <i class="fa-solid fa-box-archive fa-fw"></i>
            <span>&ensp;Archives</span>
        </a>
        
        <a href="/categories/%E6%9D%82%E8%B0%88">
            <i class="fa-solid fa-bookmark fa-fw"></i>
            <span>&ensp;Categories</span>
        </a>
        
        <a href="/tags/%E6%9D%82%E8%B0%88">
            <i class="fa-solid fa-tags fa-fw"></i>
            <span>&ensp;Tags</span>
        </a>
        
        <a href="/friends">
            <i class="fa-solid fa-link fa-fw"></i>
            <span>&ensp;Links</span>
        </a>
        
    </nav>
    <nav id="mobile-menu">
        <div class="title" @click="showMenuItems = !showMenuItems">
            <i class="fa-solid fa-bars fa-fw"></i>
            <span>&emsp;COLD RAIN&#39;S BLOG</span>
        </div>
        <transition name="slide">
            <div class="items" v-show="showMenuItems">
                
                <a href="/">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-house fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Homepage</div>
                    </div>
                </a>
                
                <a href="/2024/11/26/test">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-id-card fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">About</div>
                    </div>
                </a>
                
                <a href="/archives">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-box-archive fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Archives</div>
                    </div>
                </a>
                
                <a href="/categories/%E6%9D%82%E8%B0%88">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-bookmark fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Categories</div>
                    </div>
                </a>
                
                <a href="/tags/%E6%9D%82%E8%B0%88">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-tags fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Tags</div>
                    </div>
                </a>
                
                <a href="/friends">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-link fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Links</div>
                    </div>
                </a>
                
            </div>
        </transition>
    </nav>
</div>
<transition name="fade">
    <div id="menu-curtain" @click="showMenuItems = !showMenuItems" v-show="showMenuItems"></div>
</transition>

        <div id="main" :class="loading ? 'into-enter-from': 'into-enter-active'">
            <div class="article">
    
    
        <!-- post 部分设计 -->

        <div class="article-wrap">

            
            <div class="article-content">
                
                <div>
                    <h1>强化学习理论笔记1——基本概念</h1>
                </div>

                <div class="info">
                    <span class="date">
                        <span class="icon">
                            <i class="fa-solid fa-calendar fa-fw"></i>
                        </span>
                        2024/11/26
                    </span>

                    
                        <span class="category">
                            <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">
                                <span class="icon">
                                    <i class="fa-solid fa-bookmark fa-fw"></i>
                                </span>
                                强化学习
                            </a>
                        </span>
                    

                    
                        <span class="tags">
                            <span class="icon">
                                <i class="fa-solid fa-tags fa-fw"></i>
                            </span>
                            
                            
                                <span class="tag">
                                    
                                    <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" style="color: #00bcd4">
                                        强化学习
                                    </a>
                                </span>
                            
                                <span class="tag">
                                    
                                    <a href="/tags/%E7%AC%94%E8%AE%B0/" style="color: #00a596">
                                        笔记
                                    </a>
                                </span>
                            
                                <span class="tag">
                                    
                                    <a href="/tags/%E7%BA%AF%E7%90%86%E8%AE%BA/" style="color: #ff7d73">
                                        纯理论
                                    </a>
                                </span>
                            
                        </span>
                    
                </div>

                
                    <!-- 显示文章特有的模块（如标签、分类） -->
                    <div class="content" v-pre>
                        
                        <h1><span id="1-强化学习基本概念">1. 强化学习基本概念</span></h1>
<h2><span id="11-a-grid-world-example网格世界">1.1 A grid-world example（网格世界）</span></h2>
<p><img src="./images/rl_note1/pic1.png" alt="pic1.png"></p>
<p>在本课程中，网格世界的例子将贯穿始终（如上图所示）</p>
<p>在这个网格世界中，有一个机器人在行走。</p>
<p>网格世界中的每一个小网格，它们有不同的类型，比如有的网格是accessible，即可以进入的，用白色来表示；有的网格是forbidden，即不可进入的，用黄色来表示；另外还有一个target area，即我们希望机器人进入的网格。</p>
<p>此外，这个网格世界还有一个边界。</p>
<p>不同机器人可以在相邻的网格之间移动，但不能再斜的方向进行移动。</p>
<p>我们为什么要考虑这样一个例子呢？因为这个例子非常易于理解，方便我们理解强化学习的很多概念</p>
<p>那么，强化学习在这个例子中所要完成的任务是什么呢？强化学习需要再这个例子中找到一个比较好的路径以到达目标位置，即target area。</p>
<p>在这里面，我们会遇到很多问题。比如我们该如何去定义一个路径它究竟是好是坏？这其实是一个非常核心的问题，我们在后面会详细讨论这个问题，即如何作出最好的决策。</p>
<p>下面我们便用这个例子来解释强化学习中我们遇到的第一个概念</p>
<h2><span id="12-state状态">1.2 State（状态）</span></h2>
<p>state描述的便是agent（智能体）相对于环境的状态</p>
<p>那么在grid-world example中，state指的是什么？便是location。</p>
<p><img src="./images/rl_note1/pic2.png" alt="pic2.png"></p>
<p>在上面这个网格中，一共有9个格子，即有9个location。我们便用$ s_1,s_2,…,s_9 $ 这几个字母来表示状态。这些字母本，比如$s_1$，实际上它是一个索引，它在这个网格中对应的真正的状态当然是代表二维平面的位置，即</p>
<p>$$\mathbf {s_1} = {\begin{bmatrix} x \ y \end{bmatrix}}$$</p>
<p>如果是更加复杂的问题，那么我们除了位置之外，还需要考虑速度，而如果是机器人的话，还要对加速度等其他类型的状态信息加以考虑</p>
<h2><span id="13-state-space状态空间">1.3 State space（状态空间）</span></h2>
<p>在上面我们已经了解了状态的概念。</p>
<p>如果我们把所有的状态放到一起，我们就得到了状态空间state space。</p>
<p>状态空间的本质其实是一个集合set，什么意思呢？</p>
<p>如果我们以$S$来表示状态空间，那么它就是把所有的状态$s_i$放到一起得到的一个集合，用数学表达式表达即为：</p>
<p>$$ S=^{9}_{i=1}$$</p>
<h2><span id="14-action动作">1.4 Action（动作）</span></h2>
<p>action（动作）代表的就是我们在每一个状态实际上是有一系列的可采取的行动，在grid-world example中，可采取的行动一共有五种，我们用$a_1,…,a_5$来表示：</p>
<p><img src="./images/rl_note1/pic3.png" alt="pic3.png"></p>
<blockquote>
<p>$a_1$：向上移动<br>
$a_2$：向右移动<br>
$a_3$：向下移动<br>
$a_4$：向左移动<br>
$a_5$：原地不动</p>
</blockquote>
<h2><span id="15-action-space动作空间">1.5 Action space（动作空间）</span></h2>
<p>当我们将所用的动作放在一起，我们就得到了action space（动作空间）</p>
<p>和状态空间一样，它也是一个集合：</p>
<p>$$A(s_i)={a_i}^{5}_{i=1}$$</p>
<p>这里值得指出的一点是：动作空间是依赖于状态的，也就是说在不同状态下，便会有不同的动作空间。这也就是为什么在上面这个式子中要用到“$(s_i)$”，这就表示动作空间A实际上是$s_i$的一个函数</p>
<h2><span id="16-state-transition状态转移">1.6 State transition（状态转移）</span></h2>
<h3><span id="161-什么是-state-transition">1.6.1 什么是 state transition？</span></h3>
<p>当我们采取了一个action（动作）后，我们在grid-world example中便从一个状态转移到另外一个状态，这样的过程就被称为state transition。</p>
<p><img src="./images/rl_note1/pic4.png" alt="pic4.png"></p>
<blockquote>
<p>举个例子，比如说在状态$s_1$，我们采取的动作是$a_2$（即向右移动），那么我们下一个状态就会跳到$s_2$。这样一个过程用式子表示出来即：</p>
</blockquote>
<p>$$s_{1} \xrightarrow{a_{2}} s_{2}$$</p>
<blockquote>
<p>当我们在状态$s_1$，我们采取的动作是$a_1$（即向上移动），那么由于机器人不能走出边界，所以我们的状态还是$s_1$，用式子表示出来即：</p>
</blockquote>
<p>$$s_{1} \xrightarrow{a_{1}} s_{1}$$</p>
<p>通过上面两个例子，我们知道state transition实际上是定义agent和环境的一种交互的行为，那我们在这里能否用其他的方式来定义这种交互的行为呢？首先，因为grid-world example是一种仿真，这是一个游戏，实际上我们可以任意去作定义，比如我们刚才在上面例子中是往上走的时候撞到边界了，我会被弹回到$s_1$，其实也有可能被弹到另一个地方，比如$s_4$，甚至可能会被弹到$s_7$。因为grid-world example是一个游戏，我们想怎么定义就怎么定义。</p>
<p>而在实际当中并非如此，实际情况是不允许任意定义的。</p>
<h3><span id="162-考虑复杂情况">1.6.2 考虑复杂情况</span></h3>
<p>下面我们再来关注一下forbidden area。比如说我们在 $s_5$ ，如果我们采取了动作 $a_2$，那么下一个状态是？</p>
<blockquote>
<p>情况一：forbidden area 可以进入，但是会伴随着惩罚，那么<br>
$$ s_{5} \xrightarrow{a_{2}} s_{6}$$</p>
</blockquote>
<blockquote>
<p>情况二：forbidden area 不可进入，那么<br>
$$ s_{5} \xrightarrow{a_{2}} s_{5}$$</p>
</blockquote>
<p>在这两种情况中，情况一其实更加一般化，但也更加困难。为什么呢？</p>
<p>因为如果我们把一些状态给排除掉的话，状态空间就会变小，实际上我们做搜索的时候会更加容易。</p>
<p>而如果我们认为 forbidden area 是可以进入的话，其实就会出现一些比较有意思的现象，比如说虽然进入 forbidden area 会得到惩罚，但也许进入之后，通往 target area 反而是最近的路径，所以 agent 可能会冒险进入到 forbidden area ，然后通过 forbidden area 再抵达 target area。之后我们会详细介绍这些。</p>
<h3><span id="163-state-transition-的表达方式两种">1.6.3 state transition 的表达方式（两种）</span></h3>
<p>state transition 可以用下面这种 tabular 的形式表现出来：</p>
<table>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:center">$a_1$（upwards）</th>
<th style="text-align:center">$a_2$（rightwards）</th>
<th style="text-align:center">$a_3$（downwards）</th>
<th style="text-align:center">$a_4$（leftwards）</th>
<th style="text-align:right">$a_5$（unchanged）</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">$s_1$</td>
<td style="text-align:center">$s_1$</td>
<td style="text-align:center">$s_2$</td>
<td style="text-align:center">$s_4$</td>
<td style="text-align:center">$s_1$</td>
<td style="text-align:right">$s_1$</td>
</tr>
<tr>
<td style="text-align:left">$s_2$</td>
<td style="text-align:center">$s_2$</td>
<td style="text-align:center">$s_3$</td>
<td style="text-align:center">$s_5$</td>
<td style="text-align:center">$s_1$</td>
<td style="text-align:right">$s_2$</td>
</tr>
<tr>
<td style="text-align:left">$s_3$</td>
<td style="text-align:center">$s_3$</td>
<td style="text-align:center">$s_3$</td>
<td style="text-align:center">$s_6$</td>
<td style="text-align:center">$s_2$</td>
<td style="text-align:right">$s_3$</td>
</tr>
<tr>
<td style="text-align:left">$s_4$</td>
<td style="text-align:center">$s_1$</td>
<td style="text-align:center">$s_5$</td>
<td style="text-align:center">$s_7$</td>
<td style="text-align:center">$s_4$</td>
<td style="text-align:right">$s_4$</td>
</tr>
<tr>
<td style="text-align:left">$s_5$</td>
<td style="text-align:center">$s_2$</td>
<td style="text-align:center">$s_6$</td>
<td style="text-align:center">$s_8$</td>
<td style="text-align:center">$s_4$</td>
<td style="text-align:right">$s_5$</td>
</tr>
<tr>
<td style="text-align:left">$s_6$</td>
<td style="text-align:center">$s_3$</td>
<td style="text-align:center">$s_6$</td>
<td style="text-align:center">$s_9$</td>
<td style="text-align:center">$s_5$</td>
<td style="text-align:right">$s_6$</td>
</tr>
<tr>
<td style="text-align:left">$s_7$</td>
<td style="text-align:center">$s_4$</td>
<td style="text-align:center">$s_8$</td>
<td style="text-align:center">$s_7$</td>
<td style="text-align:center">$s_7$</td>
<td style="text-align:right">$s_7$</td>
</tr>
<tr>
<td style="text-align:left">$s_8$</td>
<td style="text-align:center">$s_5$</td>
<td style="text-align:center">$s_9$</td>
<td style="text-align:center">$s_8$</td>
<td style="text-align:center">$s_7$</td>
<td style="text-align:right">$s_8$</td>
</tr>
<tr>
<td style="text-align:left">$s_9$</td>
<td style="text-align:center">$s_6$</td>
<td style="text-align:center">$s_9$</td>
<td style="text-align:center">$s_9$</td>
<td style="text-align:center">$s_8$</td>
<td style="text-align:right">$s_9$</td>
</tr>
</tbody>
</table>
<p>上面的表格中，每一行对应一个状态，每一列对应一个动作</p>
<p>然而，虽然表格看起来很直观，但是在实际生活中，它的使用是受限的，因为它只能表示确定性的情况，而现实中的情况一般是有各种各样状态的，这时候表格就无法表达了。</p>
<p>所以这个时候，我们更一般的方法是什么呢？</p>
<p>便是用 State transition probability。（这里是我们第一次把 probability 引入到强化学习理论中来）</p>
<p>这个直观表达是什么呢？比如说我目前的状态是 $s_1$ ，我作出了动作 $a_2$ ，也就是要往右移动，下一个状态就是 $s_2$。用数学表示即为：<br>
$$ p({s_2}|s_1,a_2)=1$$<br>
$$ p({s_i}|s_1,a_2)=0(i\neq2)$$</p>
<p><img src="./images/rl_note1/pic5.png" alt="pic5.png"></p>
<p>虽然在上面这个例子中我们描述为确定性的状态，但是条件概率是可以用来描述随机性状态的例子的。比如说有北风的时候，我们在 $s_1$ 状态下作出动作 $a_2$ 后，可能我们下一个状态是 $s_2$ 的概率是0.5、状态是 $s_5$ 的概率是0.5，那么我们用数学表达式表示即为：<br>
$$ p({s_2}|s_1,a_2)=0.5$$<br>
$$ p({s_5}|s_1,a_2)=0.5$$</p>
<p>由此可见，写成概率的形式比写成表格的形式更加一般化。</p>
<h2><span id="17-policy策略">1.7 Policy（策略）</span></h2>
<h3><span id="171-什么是-policy">1.7.1 什么是 Policy？</span></h3>
<p>Policy 是强化学习当中独有的一个概念。什么是 Policy ？Policy会告诉 agent 在一个 state 状态下应该作出哪一个 action 动作。</p>
<h3><span id="172-如何表示-policy两种">1.7.2 如何表示 Policy？（两种）</span></h3>
<p><img src="./images/rl_note1/pic6.png" alt="pic6.png"></p>
<p>若要直观上理解 Policy，我们是用箭头来表示的。</p>
<p>我们可以看到上图中含有9个状态，而每一个状态都对应一个箭头（$s_9$处的圆圈代表的是原地不动~）。基于这些箭头表示的策略，我们可以得到一些 path 或者叫 trajectory（如下图所示，之后会详细介绍）</p>
<p><img src="./images/rl_note1/pic7.png" alt="pic7.png"></p>
<p>用箭头来表示还是比较直观的。然而在现实当中，我们对于稍微复杂一点的情况便无法用如此直观的形式来表示，所以我们需要用一种能够描述复杂情况、一般化情况的方法，是什么呢？便是数学表达式，这里我们仍然选择条件概率。</p>
<p>我们举一个例子：针对状态 $s_1$，它的策略 $\pi$（这里的 $\pi$ 一般情况下我们是指圆周率，但是在强化学习中 $\pi$ 就统一指的是策略，一个条件概率）指定了在任何一个状态 state 下作出任何一个动作 action 的概率是多少。对于$s_1$，我们有如下表达式：<br>
$$ \pi({a_1}|{s_1})=0$$<br>
$$ \pi({a_2}|{s_1})=1$$<br>
$$ \pi({a_3}|{s_1})=0$$<br>
$$ \pi({a_4}|{s_1})=0$$<br>
$$ \pi({a_5}|{s_1})=0$$<br>
（所有策略的概率之和为1）</p>
<p>这里我们只写出了 $s_1$ ，其实对于这所有的9个状态，每一个状态都要有它对应的策略。此外，这里我们所写的均为确定性的 Policy（Deterministic Policy），也就是说 agent 在 $s_1$ 处一定会选择作出动作 $a_2$。</p>
<p>实际上，我们还存在不确定性的 Policy（Stochastic Policy），如下图所示：</p>
<p><img src="./images/rl_note1/pic8.png" alt="pic8.png"></p>
<p>用刚才所述的条件概率的形式来表示其实也非常简单，即：<br>
$$ \pi({a_1}|{s_1})=0$$<br>
$$ \pi({a_2}|{s_1})=0.5$$<br>
$$ \pi({a_3}|{s_1})=0.5$$<br>
$$ \pi({a_4}|{s_1})=0$$<br>
$$ \pi({a_5}|{s_1})=0$$</p>
<p>当然，这样的策略也可以用表格的形式来表达出来：</p>
<table>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:center">$a_1$（upwards）</th>
<th style="text-align:center">$a_2$（rightwards）</th>
<th style="text-align:center">$a_3$（downwards）</th>
<th style="text-align:center">$a_4$（leftwards）</th>
<th style="text-align:center">$a_5$（unchanged）</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">$s_1$</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0.5</td>
<td style="text-align:center">0.5</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:left">$s_2$</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:left">$s_3$</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:left">$s_4$</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:left">$s_5$</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:left">$s_6$</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:left">$s_7$</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:left">$s_8$</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:left">$s_9$</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
</tr>
</tbody>
</table>
<p>（但是这里我们还是不推荐使用表格，比较没法表示更一般的形式）</p>
<h2><span id="18-reward奖励">1.8 Reward（奖励）</span></h2>
<h3><span id="181-什么是-reward">1.8.1 什么是 Reward？</span></h3>
<p>Reward 是强化学习中特有的一个概念。</p>
<p>首先 Reward 是一个数，是一个实数、标量，它是在 agent 采取一定动作后得到的。</p>
<p>这个数是有一定含义的：如果这个数是正数，则代表我们对这样的动作是鼓励的，而如果这个数是一个负数，代表我们不希望这样的行为发生，或者是对这个行为的一个惩罚。</p>
<p>这里面我们会遇到两个问题：</p>
<ol>
<li>首先，如果我不设置 reward，或者将 reward 设置为0的话会代表什么呢？如果深究的话可能会有些复杂，笼统地说就是代表没有 punishment，而没有 punishment 在一定程度上就是鼓励。</li>
<li>还有一个问题是，我们能否用一个正数来代表 punishment，用负数来代表鼓励？实际上是可以的，而这也是数学上的一种技巧，本质上来说得到的东西是一样的。</li>
</ol>
<h3><span id="182-如何设计-reward">1.8.2 如何设计 Reward？</span></h3>
<p><img src="./images/rl_note1/pic9.png" alt="picr9.png"></p>
<p>这里我们还是举 grid-world example  的例子。</p>
<blockquote>
<p>如果 agent 想要逃出四周的边界，每次它有这样的动作的时候，就令 $r_{bound}=-1$<br>
如果 agent 想要进入 forbidden area ，那么每次它有这样的动作的时候，就令 $r_{forbid}=-1$<br>
如果 agent 到达了目标单元格，即 target cell，就令 $r_{target}=+1$<br>
其他所有的 agent 采取的动作，我们令 $r=0$</p>
</blockquote>
<p>Reward 实际上可以被理解为一个 human-machine interface，即我们和机器进行交互的一种手段，因为 reward 设计还是相对比较直观的，所以我们可以借此引导 agent 应该怎样做、不应该怎样做。比如说在上面的例子中，通过设计奖励机制，便可以引导机器人不进入forbidden area 或是出界</p>
<h3><span id="183-如何表示-reward">1.8.3 如何表示 Reward？</span></h3>
<p>当然我们也可以用一个表格来表示 reward，如下所示：</p>
<table>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:center">$a_1$（upwards）</th>
<th style="text-align:center">$a_2$（rightwards）</th>
<th style="text-align:center">$a_3$（downwards）</th>
<th style="text-align:center">$a_4$（leftwards）</th>
<th style="text-align:right">$a_5$（unchanged）</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">$s_1$</td>
<td style="text-align:center">$r_{bound}$</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">$r_{bound}$</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td style="text-align:left">$s_2$</td>
<td style="text-align:center">$r_{bound}$</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td style="text-align:left">$s_3$</td>
<td style="text-align:center">$r_{bound}$</td>
<td style="text-align:center">$r_{bound}$</td>
<td style="text-align:center">$r_{forbid}$</td>
<td style="text-align:center">0</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td style="text-align:left">$s_4$</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">$r_{forbid}$</td>
<td style="text-align:center">$r_{bound}$</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td style="text-align:left">$s_5$</td>
<td style="text-align:center">0</td>
<td style="text-align:center">$r_{forbid}$</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td style="text-align:left">$s_6$</td>
<td style="text-align:center">0</td>
<td style="text-align:center">$r_{bound}$</td>
<td style="text-align:center">$r_{target}$</td>
<td style="text-align:center">0</td>
<td style="text-align:right">$r_{forbid}$</td>
</tr>
<tr>
<td style="text-align:left">$s_7$</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">$r_{bound}$</td>
<td style="text-align:center">$r_{bound}$</td>
<td style="text-align:right">$r_{forbid}$</td>
</tr>
<tr>
<td style="text-align:left">$s_8$</td>
<td style="text-align:center">0</td>
<td style="text-align:center">$r_{target}$</td>
<td style="text-align:center">$r_{bound}$</td>
<td style="text-align:center">$r_{forbid}$</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td style="text-align:left">$s_9$</td>
<td style="text-align:center">$r_{forbid}$</td>
<td style="text-align:center">$r_{bound}$</td>
<td style="text-align:center">$r_{bound}$</td>
<td style="text-align:center">0</td>
<td style="text-align:right">$r_{target}$</td>
</tr>
</tbody>
</table>
<p>同样的，表格表示的话会有一个问题：它的应用还是比较有限的，因为表格只能表示确定性的情况，而实际上我们会遇到很多不确定的状态。</p>
<p><img src="./images/rl_note1/pic10.png" alt="pic10.png"></p>
<p>因此，我们会采用条件概率的方法来表示更加一般化的情况。</p>
<p>比如说，我们举个例子：在状态 $s_1$，如果我们选择了动作 $a_1$，那么我们得到的reward就是-1，用数学表达式表示出来即：</p>
<p>$$ p(r=-1|s_1,a_1)=1$$<br>
$$ p(r \neq -1|s_1,a_1)=0$$</p>
<p>在这里，我们有以下几点需要注意：</p>
<blockquote>
<p>在现实中还会存在 stochastic（随机）的奖励。比如说我们学习地非常努力，我们一定会得到一个奖励，但具体可以得到多少是具有一定随机性的<br>
Reward 一定是依赖于当前的状态 state 和动作 action，而不是依赖于下一个状态</p>
</blockquote>
<h2><span id="19-trajectory">1.9 Trajectory</span></h2>
<h3><span id="191-什么是trajectory">1.9.1 什么是Trajectory？</span></h3>
<p><img src="./images/rl_note1/pic9.png" alt="pic9.png"></p>
<p>Trajectory 实际上是一个 state-action-reward 的链</p>
<p>如上图所示的 trajectory 可以表示为：<br>
$$ s_{1} \xrightarrow[r=0]{a_{2}} s_{2} \xrightarrow[r=0]{a_3} s_5 \xrightarrow[r=0]{a_3} s_8 \xrightarrow[r=1]{a_2} s_9$$</p>
<h3><span id="192-return">1.9.2 return</span></h3>
<p>这里我们还要介绍一个非常重要的概念——return。</p>
<p>一条 trajectory 的 return 即是将这条 trajectory 中的所有奖励 reward 相加所得的和，如下所示：<br>
$$ return = 0+0+0+1=1$$</p>
<h3><span id="193-比较trajectory优劣">1.9.3 比较trajectory优劣</span></h3>
<p>这里我们来看另外一条 trajectory，如下图所示：</p>
<p><img src="./images/rl_note1/pic10.png" alt="pic10.png"></p>
<p>将上图中的 trajectory 表示出来即为<br>
$$ s_{1} \xrightarrow[r=0]{a_{3}} s_{4} \xrightarrow[r=-1]{a_3} s_7 \xrightarrow[r=0]{a_2} s_8 \xrightarrow[r=1]{a_2} s_9$$<br>
这条 trajectory 的 return 为<br>
$$return = 0-1+0+1=0$$</p>
<p>那么在这里，这条 trajectory 和上文1.9.1中所介绍的 trajectory 相比，究竟哪一个 policy 比较好呢？其实直观上来说，我们会觉得第一个 policy 更好，毕竟第一个 policy 没进入到 forbidden area。但实际上我们应该从数学上来思考这个问题：因为第一个 trajectory 的 reward 更高，故其更好。</p>
<h3><span id="194-discounted-return">1.9.4 Discounted return</span></h3>
<p><img src="./images/rl_note1/pic11.png" alt="pic11.png"></p>
<p>这里我们考虑一个特殊的情况，如上图所示。图中的 trajectory 表示出来即为：<br>
$$s_{1} \xrightarrow[r=0]{a_{2}} s_{2} \xrightarrow[r=0]{a_3} s_5 \xrightarrow[r=0]{a_3} s_8 \xrightarrow[r=1]{a_2} s_9 \xrightarrow[r=1]{a_5} s_9 \xrightarrow[r=1]{a_5} s_9 …$$<br>
这时，trajectory 的 reward为<br>
$$ return=0+0+0+1+1+1+…=\infty$$<br>
这里的 trajectory 是无限长的，其 return 沿着其无穷长的轨迹不断累加下去的话，return 会发散掉。那我们应该如何解决这个问题呢？</p>
<p>这里我们需要引入 discount rate $\gamma \in [0,1)$。</p>
<p>有了 discount rate，我们就可以得到 discount return，如下所示<br>
$$discount return=0+\gamma0+{\gamma}^{2}{0}+{\gamma}^{3}{1}+{\gamma}^{4}{1}+{\gamma}^{5}{1}+…={\gamma}^{3}(1+{\gamma}+{\gamma}^2+…)={\gamma}^{3}{\frac{1}{1-\gamma}}$$</p>
<p>这里我们引入 discount return 得到了什么？首先，之前我们将 reward 相加之后 return 会发散掉，而现在 return 变成了一个有限的值。其次，discount return 能够去平衡这种更远未来能够得到的 reward 和更近未来能够得到的 reward。</p>
<p>同时，这里我们通过控制 $\gamma$ 的取值便能够控制 agent 所学到的策略。简单来说，当 $\gamma$ 较小的时候，agent会变得更加<br>
近视，也就是说它会更加注重最近的一些 reward；而如果 $\gamma$ 比较大的时候它会变得更加远视。</p>
<h2><span id="110-episode">1.10 Episode</span></h2>
<p>类似于深度学习中的轮数，用于区分 terminal states 和 continuing tasks</p>
<h2><span id="111-mdpmarkov-decision-process-马尔科夫链">1.11 MDP（Markov decision process 马尔科夫链）</span></h2>
<p>到此为止，我们已经将强化学习中一些比较基本的概念，通过例子的形式介绍完了。接下来我们需要把这些概念放到 MDP 框架当中，用更加正式的方式来重新介绍一下这些概念。</p>
<p>这里直接放图来解释（其实因为我太懒了…）：</p>
<p><img src="./images/rl_note1/pic12.png" alt="pic12.png"></p>
<p><img src="./images/rl_note1/pic13.png" alt="pic13.png"></p>
<p>小结：</p>
<p><img src="./images/rl_note1/pic14.png" alt="pic14.png"></p>

                    </div>
                
            </div>

            <div class="article-toc">
                
                    <div id="post-toc-card">
                        <div id="toc-card-style">
    <div id="toc-card-div">
        <div class="the-toc">
            
        <div id='toc'>
            <strong class="sidebar-title"> 目录 </strong>
            <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">1. 强化学习基本概念</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">1.1 A grid-world example（网格世界）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">1.2 State（状态）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">1.3 State space（状态空间）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">1.4 Action（动作）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">1.5 Action space（动作空间）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">1.6 State transition（状态转移）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">1.6.1 什么是 state transition？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">1.6.2 考虑复杂情况</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">1.6.3 state transition 的表达方式（两种）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">1.7 Policy（策略）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">1.7.1 什么是 Policy？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">1.7.2 如何表示 Policy？（两种）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">1.8 Reward（奖励）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">1.8.1 什么是 Reward？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">1.8.2 如何设计 Reward？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">1.8.3 如何表示 Reward？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">1.9 Trajectory</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">1.9.1 什么是Trajectory？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">1.9.2 return</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">1.9.3 比较trajectory优劣</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">1.9.4 Discounted return</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">1.10 Episode</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">1.11 MDP（Markov decision process 马尔科夫链）</span></a></li></ol></li></ol>
        </div>
    
        </div>
    </div>
</div>
                    </div>
                
            </div>
        </div>

        <!-- 这里插入评论区和页脚 -->
        
            
            
            
            
        


    

</div>
            <footer id="footer">
    <div id="footer-wrap">
        <div>
            &copy;
            2022 - 2026 Cold Rain&#39;s Blog
            <span id="footer-icon">
                <i class="fa-solid fa-font-awesome fa-fw"></i>
            </span>
            &commat;ColdRain
        </div>
        <div>
            Based on the <a target="_blank" rel="noopener" href="https://hexo.io">Hexo Engine</a> &amp;
            <a target="_blank" rel="noopener" href="https://github.com/theme-particlex/hexo-theme-particlex">ParticleX Theme</a>
        </div>
        
    </div>
</footer>

        </div>
        
        <transition name="fade">
            <div id="preview" ref="preview" v-show="previewShow">
                <img id="preview-content" ref="previewContent" />
            </div>
        </transition>
        
    </div>
    <script src="/js/main.js"></script>
    
    




    
</body>

<canvas
    id="background"
    style="position: fixed; top: 0; left: 0; width: 100vw; height: 100vh; pointer-events: none; z-index: -1"
></canvas>
<script src="/js/background.min.js"></script>

</html>
